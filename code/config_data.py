import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader, random_split, Subset
import numpy as np

def get_cifar100(batch_size=128, subset_ratio=1.0, augment=True):
    """
    åŠ è½½ CIFAR-100 æ•°æ®é›†ï¼ˆç»Ÿä¸€å¢å¼ºç‰ˆæœ¬ï¼‰
    æ”¯æŒ subset_ratio æ§åˆ¶é¢„è®­ç»ƒæ•°æ®è§„æ¨¡ã€‚
    """
    normalize = transforms.Normalize((0.5071, 0.4865, 0.4409),
                                     (0.2673, 0.2564, 0.2762))

    if augment:
        transform_train = transforms.Compose([
            transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(0.3, 0.3, 0.3, 0.05),
            transforms.RandomRotation(15),
            transforms.ToTensor(),
            normalize,
        ])
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        normalize,
    ])

    # âœ… åŠ è½½æ•°æ®
    train_full = datasets.CIFAR100(root="./data", train=True, download=True, transform=transform_train)
    test_set = datasets.CIFAR100(root="./data", train=False, download=True, transform=transform_test)

    # âœ… å­é›†æ§åˆ¶ï¼ˆç”¨äºæ•°æ®è§„æ¨¡å®éªŒï¼‰
    if subset_ratio < 1.0:
        subset_len = int(len(train_full) * subset_ratio)
        idx = np.random.choice(len(train_full), subset_len, replace=False)
        train_full = Subset(train_full, idx)
        print(f"ğŸ“¦ ä½¿ç”¨ {subset_len} / {len(train_full.dataset)} ({subset_ratio*100:.1f}%) æ ·æœ¬è¿›è¡Œè®­ç»ƒ")

    # âœ… è®­ç»ƒ/éªŒè¯åˆ’åˆ†
    train_size = int(0.9 * len(train_full))
    val_size = len(train_full) - train_size
    train_set, val_set = random_split(train_full, [train_size, val_size])

    return (
        DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2),
        DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2),
        DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2),
    )
